{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd \n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sk-proj-5tfVRTX5YpSUjid2qzeKT3BlbkFJWsvdi2lNHoSAQLXbngv1'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(openai_api_key=KEY, model_name= 'gpt-3.5-turbo', temperature=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=APIRemovedInV1Proxy, model_name='gpt-3.5-turbo', temperature=0.5, model_kwargs={}, openai_api_key='sk-proj-5tfVRTX5YpSUjid2qzeKT3BlbkFJWsvdi2lNHoSAQLXbngv1', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import PyPDF2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESPONSE_JSON = {\n",
    "    '1' : {\n",
    "        'mcq': 'multiple choice question',\n",
    "        'options':{\n",
    "            'a': 'choice here',\n",
    "            'b': 'choice here',\n",
    "            'c': 'choice here',\n",
    "            'd': 'choice here'\n",
    "        },\n",
    "        'correct': 'correct answer'\n",
    "            },\n",
    "    '2' : {\n",
    "        'mcq': 'multiple choice question',\n",
    "        'options':{\n",
    "            'a': 'choice here',\n",
    "            'b': 'choice here',\n",
    "            'c': 'choice here',\n",
    "            'd': 'choice here'\n",
    "        },\n",
    "        'correct': 'correct answer'\n",
    "            },\n",
    "    '3' : {\n",
    "        'mcq': 'multiple choice question',\n",
    "        'options':{\n",
    "            'a': 'choice here',\n",
    "            'b': 'choice here',\n",
    "            'c': 'choice here',\n",
    "            'd': 'choice here'\n",
    "        },\n",
    "        'correct': 'correct answer'\n",
    "            },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = \"\"\" \n",
    "Text: {text}\n",
    "You are an expert MCQ maker. Giiven the above text, it is \\\n",
    "your job to create a quiz of {number} MCQs for {subject} students \\\n",
    "in {tone} tone.\n",
    "Make sure the questions are not repeated and check all the questions\\\n",
    "to be conforming the text as well.\n",
    "Make sure to format your response like RESPONSE_JSON below and \\\n",
    "use it as a guide. Ensure to make {number} MCQs.\n",
    "# RESPONSE_JSON\n",
    "{response_json}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_generation_prompt = PromptTemplate(\n",
    "    input_variables=['text','number','subject','tone','response_json'],\n",
    "    template= TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_chain =  LLMChain(llm=llm,prompt=quiz_generation_prompt, output_key='quiz', verbose= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE2 = \"\"\" \n",
    "You are an expert english grammarian and writer. Given a Multiple \\\n",
    "Choice Question Quiz for {subject} students.\n",
    "You need to evaluate the complexity of the questions and give a \\\n",
    "complete analysis of the quiz. Only use at max 50 words for \\\n",
    "complexity analysis. if the quiz is not at par with the cognitive\\\n",
    "and analytical abilities of the students, update the quiz questions\\\n",
    "which needs to be changed and change the tone such that it \\\n",
    "perfectly fits the students abbilities.\n",
    "Quiz_MCQs:\n",
    "{quiz}\n",
    "\n",
    "Check from an expert English Writer of the above quiz:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_evaluation_prompt = PromptTemplate(\n",
    "    input_variables=['subject', 'quiz'],\n",
    "    template=TEMPLATE2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_chain = LLMChain(llm = llm, prompt=quiz_evaluation_prompt,output_key='review', verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_evaluate_chain = SequentialChain(\n",
    "    chains=[quiz_chain, review_chain],\n",
    "    input_variables=['text','number','subject','tone','response_json'],\n",
    "    output_variables=['quiz','review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Users/aws/Documents/projectmcq/data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, 'r') as file:\n",
    "    TEXT = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions.[1] Recently, artificial neural networks have been able to surpass many previous approaches in performance.[2][3]\n",
      "\n",
      "ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine.[4][5] When applied to business problems, it is known under the name predictive analytics. Although not all machine learning is statistically based, computational statistics is an important source of the field's methods.\n",
      "\n",
      "The mathematical foundations of ML are provided by mathematical optimization (mathematical programming) methods. Data mining is a related (parallel) field of study, focusing on exploratory data analysis (EDA) through unsupervised learning.[7][8]\n",
      "\n",
      "From a theoretical viewpoint, probably approximately correct (PAC) learning provides a framework for describing machine learning.\n",
      "\n",
      "SUPERVISED LEARNING : \n",
      "Supervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs.[48] The data is known as training data, and consists of a set of training examples. Each training example has one or more inputs and the desired output, also known as a supervisory signal. In the mathematical model, each training example is represented by an array or vector, sometimes called a feature vector, and the training data is represented by a matrix. Through iterative optimization of an objective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs.[49] An optimal function allows the algorithm to correctly determine the output for inputs that were not a part of the training data. An algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task.[19]\n",
      "\n",
      "UNSUPERVISED LEARNING :\n",
      "Unsupervised learning algorithms find structures in data that has not been labeled, classified or categorized. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. Central applications of unsupervised machine learning include clustering, dimensionality reduction,[8] and density estimation.[51] Unsupervised learning algorithms also streamlined the process of identifying large indel based haplotypes of a gene of interest from pan-genome.[52]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"1\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"2\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"3\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}}'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converting python dict into a json formatted string\n",
    "json.dumps(RESPONSE_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER = 5\n",
    "SUBJECT = 'machine learning'\n",
    "TONE = 'simple'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m \n",
      "Text: Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions.[1] Recently, artificial neural networks have been able to surpass many previous approaches in performance.[2][3]\n",
      "\n",
      "ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine.[4][5] When applied to business problems, it is known under the name predictive analytics. Although not all machine learning is statistically based, computational statistics is an important source of the field's methods.\n",
      "\n",
      "The mathematical foundations of ML are provided by mathematical optimization (mathematical programming) methods. Data mining is a related (parallel) field of study, focusing on exploratory data analysis (EDA) through unsupervised learning.[7][8]\n",
      "\n",
      "From a theoretical viewpoint, probably approximately correct (PAC) learning provides a framework for describing machine learning.\n",
      "\n",
      "SUPERVISED LEARNING : \n",
      "Supervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs.[48] The data is known as training data, and consists of a set of training examples. Each training example has one or more inputs and the desired output, also known as a supervisory signal. In the mathematical model, each training example is represented by an array or vector, sometimes called a feature vector, and the training data is represented by a matrix. Through iterative optimization of an objective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs.[49] An optimal function allows the algorithm to correctly determine the output for inputs that were not a part of the training data. An algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task.[19]\n",
      "\n",
      "UNSUPERVISED LEARNING :\n",
      "Unsupervised learning algorithms find structures in data that has not been labeled, classified or categorized. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. Central applications of unsupervised machine learning include clustering, dimensionality reduction,[8] and density estimation.[51] Unsupervised learning algorithms also streamlined the process of identifying large indel based haplotypes of a gene of interest from pan-genome.[52]\n",
      "\n",
      "\n",
      "You are an expert MCQ maker. Giiven the above text, it is your job to create a quiz of 5 MCQs for machine learning students in simple tone.\n",
      "Make sure the questions are not repeated and check all the questionsto be conforming the text as well.\n",
      "Make sure to format your response like RESPONSE_JSON below and use it as a guide. Ensure to make 5 MCQs.\n",
      "# RESPONSE_JSON\n",
      "{\"1\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"2\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"3\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}}\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'openai' has no attribute 'error'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/aws/Documents/projectmcq/experiment/mcq.ipynb Cell 22\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aws/Documents/projectmcq/experiment/mcq.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# setting up token usgae tracking in langchain\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aws/Documents/projectmcq/experiment/mcq.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mwith\u001b[39;00m get_openai_callback() \u001b[39mas\u001b[39;00m cb:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/aws/Documents/projectmcq/experiment/mcq.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     response \u001b[39m=\u001b[39m generate_evaluate_chain(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aws/Documents/projectmcq/experiment/mcq.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         {\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aws/Documents/projectmcq/experiment/mcq.ipynb#X26sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m             \u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m: TEXT,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aws/Documents/projectmcq/experiment/mcq.ipynb#X26sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m             \u001b[39m'\u001b[39;49m\u001b[39mnumber\u001b[39;49m\u001b[39m'\u001b[39;49m: NUMBER,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aws/Documents/projectmcq/experiment/mcq.ipynb#X26sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m             \u001b[39m'\u001b[39;49m\u001b[39msubject\u001b[39;49m\u001b[39m'\u001b[39;49m : SUBJECT,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aws/Documents/projectmcq/experiment/mcq.ipynb#X26sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m             \u001b[39m'\u001b[39;49m\u001b[39mtone\u001b[39;49m\u001b[39m'\u001b[39;49m : TONE,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/aws/Documents/projectmcq/experiment/mcq.ipynb#X26sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m             \u001b[39m'\u001b[39;49m\u001b[39mresponse_json\u001b[39;49m\u001b[39m'\u001b[39;49m : json\u001b[39m.\u001b[39;49mdumps(RESPONSE_JSON)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aws/Documents/projectmcq/experiment/mcq.ipynb#X26sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         }\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/aws/Documents/projectmcq/experiment/mcq.ipynb#X26sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m            )\n",
      "File \u001b[0;32m~/Documents/projectmcq/myenv/lib/python3.8/site-packages/langchain/chains/base.py:243\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    242\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 243\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    244\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    245\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    246\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    247\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/projectmcq/myenv/lib/python3.8/site-packages/langchain/chains/base.py:237\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    231\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    232\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    233\u001b[0m     inputs,\n\u001b[1;32m    234\u001b[0m )\n\u001b[1;32m    235\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    236\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 237\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    238\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    239\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    240\u001b[0m     )\n\u001b[1;32m    241\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    242\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/Documents/projectmcq/myenv/lib/python3.8/site-packages/langchain/chains/sequential.py:102\u001b[0m, in \u001b[0;36mSequentialChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39mfor\u001b[39;00m i, chain \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchains):\n\u001b[1;32m    101\u001b[0m     callbacks \u001b[39m=\u001b[39m _run_manager\u001b[39m.\u001b[39mget_child()\n\u001b[0;32m--> 102\u001b[0m     outputs \u001b[39m=\u001b[39m chain(known_values, return_only_outputs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, callbacks\u001b[39m=\u001b[39;49mcallbacks)\n\u001b[1;32m    103\u001b[0m     known_values\u001b[39m.\u001b[39mupdate(outputs)\n\u001b[1;32m    104\u001b[0m \u001b[39mreturn\u001b[39;00m {k: known_values[k] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_variables}\n",
      "File \u001b[0;32m~/Documents/projectmcq/myenv/lib/python3.8/site-packages/langchain/chains/base.py:243\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    242\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 243\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    244\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    245\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    246\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    247\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/projectmcq/myenv/lib/python3.8/site-packages/langchain/chains/base.py:237\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    231\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    232\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    233\u001b[0m     inputs,\n\u001b[1;32m    234\u001b[0m )\n\u001b[1;32m    235\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    236\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 237\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    238\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    239\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    240\u001b[0m     )\n\u001b[1;32m    241\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    242\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/Documents/projectmcq/myenv/lib/python3.8/site-packages/langchain/chains/llm.py:92\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\n\u001b[1;32m     88\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     89\u001b[0m     inputs: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[1;32m     90\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     91\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m---> 92\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate([inputs], run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m     93\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/projectmcq/myenv/lib/python3.8/site-packages/langchain/chains/llm.py:102\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m    101\u001b[0m prompts, stop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_prompts(input_list, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[0;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mgenerate_prompt(\n\u001b[1;32m    103\u001b[0m     prompts,\n\u001b[1;32m    104\u001b[0m     stop,\n\u001b[1;32m    105\u001b[0m     callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child() \u001b[39mif\u001b[39;49;00m run_manager \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    106\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_kwargs,\n\u001b[1;32m    107\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/projectmcq/myenv/lib/python3.8/site-packages/langchain/chat_models/base.py:230\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[1;32m    223\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    224\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    228\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    229\u001b[0m     prompt_messages \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_messages() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 230\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_messages, stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/projectmcq/myenv/lib/python3.8/site-packages/langchain/chat_models/base.py:125\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n\u001b[1;32m    124\u001b[0m             run_managers[i]\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 125\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    126\u001b[0m flattened_outputs \u001b[39m=\u001b[39m [\n\u001b[1;32m    127\u001b[0m     LLMResult(generations\u001b[39m=\u001b[39m[res\u001b[39m.\u001b[39mgenerations], llm_output\u001b[39m=\u001b[39mres\u001b[39m.\u001b[39mllm_output)\n\u001b[1;32m    128\u001b[0m     \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results\n\u001b[1;32m    129\u001b[0m ]\n\u001b[1;32m    130\u001b[0m llm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combine_llm_outputs([res\u001b[39m.\u001b[39mllm_output \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/Documents/projectmcq/myenv/lib/python3.8/site-packages/langchain/chat_models/base.py:115\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39mfor\u001b[39;00m i, m \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(messages):\n\u001b[1;32m    113\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m         results\u001b[39m.\u001b[39mappend(\n\u001b[0;32m--> 115\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_with_cache(\n\u001b[1;32m    116\u001b[0m                 m,\n\u001b[1;32m    117\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    118\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[i] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    119\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    120\u001b[0m             )\n\u001b[1;32m    121\u001b[0m         )\n\u001b[1;32m    122\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    123\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/Documents/projectmcq/myenv/lib/python3.8/site-packages/langchain/chat_models/base.py:262\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    259\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    260\u001b[0m     )\n\u001b[1;32m    261\u001b[0m \u001b[39mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 262\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    263\u001b[0m         messages, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    264\u001b[0m     )\n\u001b[1;32m    265\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    266\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(messages, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Documents/projectmcq/myenv/lib/python3.8/site-packages/langchain/chat_models/openai.py:371\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    363\u001b[0m     message \u001b[39m=\u001b[39m _convert_dict_to_message(\n\u001b[1;32m    364\u001b[0m         {\n\u001b[1;32m    365\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: inner_completion,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    368\u001b[0m         }\n\u001b[1;32m    369\u001b[0m     )\n\u001b[1;32m    370\u001b[0m     \u001b[39mreturn\u001b[39;00m ChatResult(generations\u001b[39m=\u001b[39m[ChatGeneration(message\u001b[39m=\u001b[39mmessage)])\n\u001b[0;32m--> 371\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompletion_with_retry(messages\u001b[39m=\u001b[39;49mmessage_dicts, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    372\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m~/Documents/projectmcq/myenv/lib/python3.8/site-packages/langchain/chat_models/openai.py:313\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompletion_with_retry\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    312\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 313\u001b[0m     retry_decorator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_retry_decorator()\n\u001b[1;32m    315\u001b[0m     \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    316\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    317\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Documents/projectmcq/myenv/lib/python3.8/site-packages/langchain/chat_models/openai.py:302\u001b[0m, in \u001b[0;36mChatOpenAI._create_retry_decorator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    294\u001b[0m max_seconds \u001b[39m=\u001b[39m \u001b[39m60\u001b[39m\n\u001b[1;32m    295\u001b[0m \u001b[39m# Wait 2^x * 1 second between each retry starting with\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[39m# 4 seconds, then up to 10 seconds, then 10 seconds afterwards\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[39mreturn\u001b[39;00m retry(\n\u001b[1;32m    298\u001b[0m     reraise\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    299\u001b[0m     stop\u001b[39m=\u001b[39mstop_after_attempt(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_retries),\n\u001b[1;32m    300\u001b[0m     wait\u001b[39m=\u001b[39mwait_exponential(multiplier\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mmin\u001b[39m\u001b[39m=\u001b[39mmin_seconds, \u001b[39mmax\u001b[39m\u001b[39m=\u001b[39mmax_seconds),\n\u001b[1;32m    301\u001b[0m     retry\u001b[39m=\u001b[39m(\n\u001b[0;32m--> 302\u001b[0m         retry_if_exception_type(openai\u001b[39m.\u001b[39;49merror\u001b[39m.\u001b[39mTimeout)\n\u001b[1;32m    303\u001b[0m         \u001b[39m|\u001b[39m retry_if_exception_type(openai\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mAPIError)\n\u001b[1;32m    304\u001b[0m         \u001b[39m|\u001b[39m retry_if_exception_type(openai\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mAPIConnectionError)\n\u001b[1;32m    305\u001b[0m         \u001b[39m|\u001b[39m retry_if_exception_type(openai\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mRateLimitError)\n\u001b[1;32m    306\u001b[0m         \u001b[39m|\u001b[39m retry_if_exception_type(openai\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mServiceUnavailableError)\n\u001b[1;32m    307\u001b[0m     ),\n\u001b[1;32m    308\u001b[0m     before_sleep\u001b[39m=\u001b[39mbefore_sleep_log(logger, logging\u001b[39m.\u001b[39mWARNING),\n\u001b[1;32m    309\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'openai' has no attribute 'error'"
     ]
    }
   ],
   "source": [
    "# setting up token usgae tracking in langchain\n",
    "with get_openai_callback() as cb:\n",
    "    response = generate_evaluate_chain(\n",
    "        {\n",
    "            'text': TEXT,\n",
    "            'number': NUMBER,\n",
    "            'subject' : SUBJECT,\n",
    "            'tone' : TONE,\n",
    "            'response_json' : json.dumps(RESPONSE_JSON)\n",
    "        }\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
